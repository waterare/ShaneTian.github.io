<!DOCTYPE html>
<html lang="zh-Hans">
<head>

    <!--[if lt IE 9]>
        <style>body {display: none; background: none !important} </style>
        <meta http-equiv="Refresh" Content="0; url=//outdatedbrowser.com/" />
    <![endif]-->

<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta name="format-detection" content="telephone=no" />
<meta name="author" content="Suixin" />



<meta name="description" content="Transformer是个叠加的“自注意力机制（Self Attention）”构成的深度网络，是目前NLP里最强的特征提取器。论文：Attention Is All You Need">
<meta name="keywords" content="深度学习,NLP">
<meta property="og:type" content="article">
<meta property="og:title" content="The Transformer">
<meta property="og:url" content="https://suixinblog.cn/2019/01/transformer.html">
<meta property="og:site_name" content="Suixin&#39;s Blog">
<meta property="og:description" content="Transformer是个叠加的“自注意力机制（Self Attention）”构成的深度网络，是目前NLP里最强的特征提取器。论文：Attention Is All You Need">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://ws3.sinaimg.cn/large/006tNc79ly1fz9wzpgnsxj30g50iaaat.jpg">
<meta property="og:image" content="https://ws4.sinaimg.cn/large/006tNc79ly1fz9wzvwzxgj30ot09p3yw.jpg">
<meta property="og:image" content="https://ws4.sinaimg.cn/large/006tNc79ly1fzajx9mh42j30kw0420tb.jpg">
<meta property="og:image" content="https://ws1.sinaimg.cn/large/006tNc79ly1fzaqej0qupj30k8028aa8.jpg">
<meta property="og:image" content="https://ws4.sinaimg.cn/large/006tNc79ly1fzaqigx5gwj30js0ii0ur.jpg">
<meta property="og:image" content="https://ws4.sinaimg.cn/large/006tNc79ly1fzaule5ofkj313b0mc7a2.jpg">
<meta property="og:image" content="https://ws1.sinaimg.cn/large/006tNc79ly1fzaujsgus8g30m80jo4ni.gif">
<meta property="og:image" content="https://ws2.sinaimg.cn/large/006tNc79ly1fzaustjxg4j30fw0jcq4s.jpg">
<meta property="og:updated_time" content="2019-01-18T10:25:11.235Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="The Transformer">
<meta name="twitter:description" content="Transformer是个叠加的“自注意力机制（Self Attention）”构成的深度网络，是目前NLP里最强的特征提取器。论文：Attention Is All You Need">
<meta name="twitter:image" content="https://ws3.sinaimg.cn/large/006tNc79ly1fz9wzpgnsxj30g50iaaat.jpg">

<link rel="apple-touch-icon" href= "/apple-touch-icon.png">


    <link rel="alternate" href="/atom.xml" title="Suixin&#39;s Blog" type="application/atom+xml">



    <link rel="shortcut icon" href="/favicon.ico">



    <link href="//cdn.bootcss.com/animate.css/3.5.1/animate.min.css" rel="stylesheet">



    <link href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.css" rel="stylesheet">



    <script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
    <link href="//cdn.bootcss.com/pace/1.0.2/themes/blue/pace-theme-minimal.css" rel="stylesheet">


<link rel="stylesheet" href="/css/style.css">



<link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">


<title>The Transformer | Suixin&#39;s Blog</title>

<script src="//cdn.bootcss.com/jquery/2.2.4/jquery.min.js"></script>
<script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script>

<script>
    var yiliaConfig = {
        fancybox: true,
        animate: true,
        isHome: false,
        isPost: true,
        isArchive: false,
        isTag: false,
        isCategory: false,
        fancybox_js: "//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.js",
        scrollreveal: "//cdn.bootcss.com/scrollReveal.js/3.1.4/scrollreveal.min.js",
        search: true
    }
</script>


    <script>
        yiliaConfig.jquery_ui = [true, "//cdn.bootcss.com/jqueryui/1.10.4/jquery-ui.min.js", "//cdn.bootcss.com/jqueryui/1.10.4/css/jquery-ui.min.css"];
    </script>



    <script> yiliaConfig.rootUrl = "\/";</script>






</head>
<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
    <header id="header" class="inner">
        <a href="/" class="profilepic">
            <img src="/img/avatar.png" class="animated zoomIn">
        </a>
        <hgroup>
          <h1 class="header-author"><a href="/">Suixin</a></h1>
        </hgroup>

        
        <p class="header-subtitle">路在脚下，心向远方</p>
        

        
            <form id="search-form">
            <input type="text" id="local-search-input" name="q" placeholder="search..." class="search form-control" autocomplete="off" autocorrect="off" searchonload="false" />
            <i class="fa fa-times" onclick="resetSearch()"></i>
            </form>
            <div id="local-search-result"></div>
            <p class='no-result'>No results found <i class='fa fa-spinner fa-pulse'></i></p>
        


        
            <div id="switch-btn" class="switch-btn">
                <div class="icon">
                    <div class="icon-ctn">
                        <div class="icon-wrap icon-house" data-idx="0">
                            <div class="birdhouse"></div>
                            <div class="birdhouse_holes"></div>
                        </div>
                        <div class="icon-wrap icon-ribbon hide" data-idx="1">
                            <div class="ribbon"></div>
                        </div>
                        
                        <div class="icon-wrap icon-link hide" data-idx="2">
                            <div class="loopback_l"></div>
                            <div class="loopback_r"></div>
                        </div>
                        
                        
                        <div class="icon-wrap icon-me hide" data-idx="3">
                            <div class="user"></div>
                            <div class="shoulder"></div>
                        </div>
                        
                    </div>
                    
                </div>
                <div class="tips-box hide">
                    <div class="tips-arrow"></div>
                    <ul class="tips-inner">
                        <li>菜单</li>
                        <li>标签</li>
                        
                        <li>友情链接</li>
                        
                        
                        <li>关于我</li>
                        
                    </ul>
                </div>
            </div>
        

        <div id="switch-area" class="switch-area">
            <div class="switch-wrap">
                <section class="switch-part switch-part1">
                    <nav class="header-menu">
                        <ul>
                        
                            <li><a href="/">主页</a></li>
                        
                            <li><a href="/archives/">所有文章</a></li>
                        
                            <li><a href="/tags/">分类&amp;标签</a></li>
                        
                            <li><a href="/about/">关于</a></li>
                        
                        </ul>
                    </nav>
                    <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" href="mailto:tianxinyqq@163.com" title="Email"></a>
                            
                                <a class="fa GitHub" href="https://github.com/ShaneTian" title="GitHub"></a>
                            
                                <a class="fa CSDN" href="https://me.csdn.net/weixin_43269020" title="CSDN"></a>
                            
                                <a class="fa 简书" href="https://www.jianshu.com/u/f5edebf2fdd6" title="简书"></a>
                            
                        </ul>
                    </nav>
                </section>
                
                
                <section class="switch-part switch-part2">
                    <div class="widget tagcloud" id="js-tagcloud">
                        <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Anaconda/">Anaconda</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Excel/">Excel</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hexo/">Hexo</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hugo/">Hugo</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LaTex/">LaTex</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NLP/">NLP</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/">Python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/dict/">dict</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/git/">git</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/json/">json</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/list/">list</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/分类/">分类</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/命令行/">命令行</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/服务器/">服务器</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/机器学习/">机器学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/正则表达式/">正则表达式</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/深度学习/">深度学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/网站建设/">网站建设</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/踩坑/">踩坑</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/面试/">面试</a></li></ul>
                    </div>
                </section>
                
                
                
                <section class="switch-part switch-part3">
                    <div id="js-friends">
                    
                      <a class="main-nav-link switch-friends-link" href="http://moxfive.xyz/">MOxFIVE</a>
                    
                      <a class="main-nav-link switch-friends-link" href="https://www.weiweiblog.cn/">尾尾部落</a>
                    
                      <a class="main-nav-link switch-friends-link" href="https://huzhiqiang.tech/">Hu&#39;s Blog</a>
                    
                    </div>
                </section>
                

                
                
                <section class="switch-part switch-part4">
                
                    <div id="js-aboutme">理工男。爱科技，爱探索，爱吃，爱玩</div>
                </section>
                
            </div>
        </div>
    </header>                
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
      <div class="overlay">
          <div class="slider-trigger"></div>
          <h1 class="header-author js-mobile-header hide"><a href="/" title="回到主页">Suixin</a></h1>
      </div>
    <div class="intrude-less">
        <header id="header" class="inner">
            <a href="/" class="profilepic">
                <img src="/img/avatar.png" class="animated zoomIn">
            </a>
            <hgroup>
              <h1 class="header-author"><a href="/" title="回到主页">Suixin</a></h1>
            </hgroup>
            
            <p class="header-subtitle">路在脚下，心向远方</p>
            
            <nav class="header-menu">
                <ul>
                
                    <li><a href="/">主页</a></li>
                
                    <li><a href="/archives/">所有文章</a></li>
                
                    <li><a href="/tags/">分类&amp;标签</a></li>
                
                    <li><a href="/about/">关于</a></li>
                
                <div class="clearfix"></div>
                </ul>
            </nav>
            <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" target="_blank" href="mailto:tianxinyqq@163.com" title="Email"></a>
                            
                                <a class="fa GitHub" target="_blank" href="https://github.com/ShaneTian" title="GitHub"></a>
                            
                                <a class="fa CSDN" target="_blank" href="https://me.csdn.net/weixin_43269020" title="CSDN"></a>
                            
                                <a class="fa 简书" target="_blank" href="https://www.jianshu.com/u/f5edebf2fdd6" title="简书"></a>
                            
                        </ul>
            </nav>
        </header>                
    </div>
    <link class="menu-list" tags="标签" friends="友情链接" about="关于我"/>
</nav>
      <div class="body-wrap"><article id="post-The-Transformer" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2019/01/transformer.html" class="article-date">
      <time datetime="2019-01-18T10:16:53.000Z" itemprop="datePublished">2019-01-18</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      The Transformer
    </h1>
  

      </header>
      
      <div class="article-info article-info-post">
        
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/术业专攻/">术业专攻</a>
    </div>


        
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/">NLP</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/深度学习/">深度学习</a></li></ul>
    </div>

        <div class="clearfix"></div>
      </div>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>Transformer是个叠加的“自注意力机制（Self Attention）”构成的深度网络，是目前NLP里最强的特征提取器。<br>论文：<a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention Is All You Need</a><br><a id="more"></a></p>
<h2 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h2><p>整体上还是由Encoders和Decoders两部分组成的，而每一个部分是由6个Encoder和Decoder堆栈成的，每个的结构完全相同，但不共享权重。</p>
<h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><p>每个Encoder由两部分组成：Multi-head self-attention层和Feed Forward NN层。</p>
<h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><p>每个Decoder由三部分组成：Multi-head self-attention层，Encoder-Decoder Attention层和Feed Forward NN层。</p>
<h2 id="模型细节"><a href="#模型细节" class="headerlink" title="模型细节"></a>模型细节</h2><h3 id="Encoder-1"><a href="#Encoder-1" class="headerlink" title="Encoder"></a>Encoder</h3><h4 id="self-attention"><a href="#self-attention" class="headerlink" title="self-attention"></a>self-attention</h4><p>动机：当模型处理每个单词（输入序列中的每个位置）时，self-attention允许它查看输入序列中的其他位置以寻找可以帮助导致对该单词更好的编码的线索。</p>
<ol>
<li>每个单词除了本身的Embedding向量$x$都对应三个向量，分别有不同的用处：Query向量$q$，Key向量$k$和Value向量$v$。维数一般低于单词的嵌入向量，论文中使用64（Embedding为512维）。这三个向量分别是在训练过程中将Embedding向量$x$乘以三个矩阵$W^Q$，$W^K$和$W^V$得到。</li>
<li>我们需要根据当前编码的单词对输入句子的每个单词进行评分，分数决定了对输入句子的其他部分放置多少焦点。通过将当前单词的Query向量与其它单词的Key向量做内积计算得分。</li>
<li>将得分除以8（论文中使用Key向量维数的平方根）。可以使得梯度更稳定？</li>
<li>softmax层将所有的分值变为概率值。</li>
<li>用softmax概率值乘以Value向量。直观解释：保持想要关注的值更大，不想关注的单词被淹没掉。</li>
<li>对上面得到的向量求和得到当前编码单词的self-attention输出向量$z$。</li>
</ol>
<p>使用矩阵形式可以并行计算。<br><strong>图示</strong>：</p>
<center><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fz9wzpgnsxj30g50iaaat.jpg" ,="" alt="self-attention矩阵形式1" width="30%"></center>
<center><img src="https://ws4.sinaimg.cn/large/006tNc79ly1fz9wzvwzxgj30ot09p3yw.jpg" ,="" alt="self-attention矩阵形式2" width="50%"></center>

<h4 id="Multi-head"><a href="#Multi-head" class="headerlink" title="Multi-head"></a>Multi-head</h4><p>动机：将信息映射到不同的子空间，可能会抓取到不同位置的注意信息。<br>按照self-attention方式进行相同的几次计算（论文中使用8头），每次使用不同的权重矩阵（$W^Q$，$W^K$和$W^V$），最终会得到几个不同的$Z$矩阵，将它们直接拼接起来得到一个很长的矩阵$Z$，再乘以一个参数矩阵$W^O$将矩阵压缩到低维（同Embedding维数）。</p>
<h4 id="Position-encoding"><a href="#Position-encoding" class="headerlink" title="Position encoding"></a>Position encoding</h4><p>单词顺序是NLP中非常重要的信息，所以加入Position encoding是考虑输入序列中单词顺序的一种方法。将位置编码与Embedding向量直接加起来得到真正的单词输入向量。<br>论文中给出了两个位置编码公式：</p>
<center><img src="https://ws4.sinaimg.cn/large/006tNc79ly1fzajx9mh42j30kw0420tb.jpg" ,="" alt="Position encoding公式" width="40%"></center>

<p>此处，pos是当前单词所处的位置，$2i/2i+1$是Position encoding的维度信息（如第一个单词的第一维位置编码值为$PE_{(0,2\times0)}=sin(0/10000^{2\times0/512})=0$，pos和维度都是从0开始。<em>参考1中Position Encoding算错了</em>）。使用这个正弦余弦的公式的优点是可以扩展到序列长度大于训练模型中任意长度的情况。</p>
<h4 id="Feed-Forward-NN层"><a href="#Feed-Forward-NN层" class="headerlink" title="Feed Forward NN层"></a>Feed Forward NN层</h4><p>该层为简单的全连接层，使用了RELU激活函数，论文中该全连接的隐藏层维数为2048，公式如下：</p>
<center><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fzaqej0qupj30k8028aa8.jpg" ,="" alt="Feed Forward NN层公式" width="40%"></center>

<h4 id="Add-amp-Norm"><a href="#Add-amp-Norm" class="headerlink" title="Add &amp; Norm"></a>Add &amp; Norm</h4><p>在每一个子层的结束，输出矩阵为$Z$，我们将该层的输入矩阵$X$和$Z$直接相加，再做Normalize操作$LayerNorm(X+Z)$，该Norm函数引用了<a href="https://arxiv.org/pdf/1607.06450.pdf" target="_blank" rel="noopener">参考文献1: Layer Normalization</a>。<br>Norm方法有很多，但它们都有一个共同的目的，那就是把输入转化成均值为0方差为1的数据。我们在把数据送入激活函数之前进行normalization，因为我们不希望输入数据落在激活函数的饱和区。</p>
<h4 id="总架构图"><a href="#总架构图" class="headerlink" title="总架构图"></a>总架构图</h4><center><img src="https://ws4.sinaimg.cn/large/006tNc79ly1fzaqigx5gwj30js0ii0ur.jpg" ,="" alt="Encoder总架构图" width="50%"></center>

<p><strong>真正的Encoders由N（论文中N=6）个相同的Encoder堆栈而成</strong></p>
<h3 id="Decoder-1"><a href="#Decoder-1" class="headerlink" title="Decoder"></a>Decoder</h3><ol>
<li>由Encoder最后的输出矩阵变换得到两个Attention矩阵$K_{encoder}$和$V_{encoder}$，分别输入到每个Encoder-Decoder Attention层，需使用时直接查询其中的Key向量和Value向量。这有助于解码器关注输入序列中的适当位置。而此层的Query向量则由之前的Decoder层得到。</li>
<li>在每一次Decoders解码一个单词的时候，整个模型的输入有三个：网络底层输入上一个解码出单词的embedding向量，在Encoder-Decoder层输入Encoders最后输出的两个矩阵$K_{encoder}$和$V_{encoder}$。使用上一个解码出的单词embedding向量传到Decoders最底层过一遍Masked Multi-head Attention，最后输出一个向量，再用这个向量生成Query向量以供中间的Encoder-Decoder Attention层使用。</li>
<li>只有第一个Decoder需要上一个解码出单词的embedding向量信息，后面的几个Decoder直接将上一个Decoder的输出作为Masked Multi-head Attention的输入。</li>
<li>每一个Decoder也都和Encoder一样使用了Add &amp; Norm以及Position encoding。</li>
</ol>
<h4 id="Linear层和softmax层"><a href="#Linear层和softmax层" class="headerlink" title="Linear层和softmax层"></a>Linear层和softmax层</h4><p>该层是一个简单的全连接网络，将最后一个Decoder输出的向量投影到一个更高维度的空间去（词典维数）。<br>softmax层将Linear层的输出向量转化为概率输出，选择最大概率的单词作为输出。</p>
<h3 id="两张有助于理解的图"><a href="#两张有助于理解的图" class="headerlink" title="两张有助于理解的图"></a>两张有助于理解的图</h3><p>Encoders最后将$K_{encoder}$和$V_{encoder}$输出给每个Decoder的Encoder-Decoder层：</p>
<center><img src="https://ws4.sinaimg.cn/large/006tNc79ly1fzaule5ofkj313b0mc7a2.jpg" ,="" alt="架构图" width="80%"></center>
Google AI Blog做的动图：
<center><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fzaujsgus8g30m80jo4ni.gif" ,="" alt="Google AI Blog动图" width="50%"></center>

<h3 id="Mask"><a href="#Mask" class="headerlink" title="Mask"></a>Mask</h3><p>Padding mask在所有的scaled dot-product attention里面都需要用到，而Sequence mask只有在Decoder的self-attention里面用到。</p>
<h4 id="Padding-mask"><a href="#Padding-mask" class="headerlink" title="Padding mask"></a>Padding mask</h4><p>语料库中每个句子的长度是不同的，我们需要对齐。使用我们设置的阈值（一般为255），对于较长的序列，直接截取左边的序列，对于较短的序列，在其后添加0。<br>而在scaled dot-product attention中，不能对这部分添加了0的单词位置加上较高的注意力，所以在self-attention中的softmax之前，直接将这些位置的值设为$-Inf$，经过softmax后这些位置的概率值会变为0。<br>即下图中的<code>Mask(opt.)</code>块：</p>
<center><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fzaustjxg4j30fw0jcq4s.jpg" ,="" alt="Padding mask" width="30%"></center>

<h4 id="Sequence-mask"><a href="#Sequence-mask" class="headerlink" title="Sequence mask"></a>Sequence mask</h4><p>Sequence mask是为了使得Decoder不能看见未来的信息，使得解码器的attention只能关注当前解码单词之前的输出单词，而不能依赖后面未解码出来的。<br>所以跟Padding mask一样，对其后的单词位置直接设为$-Inf$，经过softmax后这些位置的概率值会变为0。<br>这步操作对应Decoder中第一个构件：Masked Multi-head Attention。</p>
<h3 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h3><p>使用交叉熵或者KL散度去比较两个输出之间的差距，然后使用反向传播优化其中的所有参数。</p>
<h4 id="beam-search（束搜索）"><a href="#beam-search（束搜索）" class="headerlink" title="beam search（束搜索）"></a>beam search（束搜索）</h4><p>在最后的softmax层我们直接输出了最大值位置的单词，叫做贪婪解码。<br>另一种更合理的解码方式叫做<strong>束搜索</strong>。假设第1#位置解码出的概率值，前两大的位置单词为<code>I</code>和<code>me</code>，那么在第2#位置解码时，依赖的第1#位置单词分别取为<code>I</code>和<code>me</code>，分别跑两次算法，在其中再选两个得分最高（或误差最小）的结果，依次类推。最终会得到两个得分最高的序列。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><blockquote>
<ol>
<li>主要参考: <a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener">The Illustrated Transformer</a></li>
<li>Google AI Blog: <a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html" target="_blank" rel="noopener">Transformer: A Novel Neural Network Architecture for Language Understanding</a></li>
<li><a href="https://www.jianshu.com/p/ef41302edeef" target="_blank" rel="noopener">神经机器翻译 之 谷歌 transformer 模型</a></li>
<li><a href="https://terrifyzhao.github.io/2019/01/11/Transformer%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3.html" target="_blank" rel="noopener">Transformer模型详解</a></li>
<li>Harvard NLP的解释和代码: <a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank" rel="noopener">The Annotated Transformer</a></li>
</ol>
</blockquote>

      
    </div>
    
  </div>
  
    
    <div class="copyright">
        <p><span>本文标题:</span><a href="/2019/01/transformer.html">The Transformer</a></p>
        <p><span>文章作者:</span><a href="/" title="回到主页">Suixin</a></p>
        <p><span>发布时间:</span>2019-01-18, 18:16:53</p>
        <p><span>最后更新:</span>2019-01-18, 18:25:11</p>
        <p>
            <span>原始链接:</span><a class="post-url" href="/2019/01/transformer.html" title="The Transformer">https://suixinblog.cn/2019/01/transformer.html</a>
            <span class="copy-path" data-clipboard-text="原文: https://suixinblog.cn/2019/01/transformer.html　　作者: Suixin" title="点击复制文章链接"><i class="fa fa-clipboard"></i></span>
            <script> var clipboard = new Clipboard('.copy-path'); </script>
        </p>
        <p>
            <span>许可协议:</span><i class="fa fa-creative-commons"></i> <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" title="CC BY-NC-SA 4.0 International" target = "_blank">"署名-非商用-相同方式共享 4.0"</a> 转载请保留原文链接及作者。
        </p>
    </div>



    <nav id="article-nav">
        
            <div id="article-nav-newer" class="article-nav-title">
                <a href="/2019/02/python-json.html">
                    利用Python处理json文件
                </a>
            </div>
        
        
            <div id="article-nav-older" class="article-nav-title">
                <a href="/2019/01/os-walk.html">
                    Python递归处理目录下的文件
                </a>
            </div>
        
    </nav>

  
</article>

    <div id="toc" class="toc-article">
        <strong class="toc-title">文章目录</strong>
        
            <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#模型架构"><span class="toc-number">1.</span> <span class="toc-text">模型架构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Encoder"><span class="toc-number">1.1.</span> <span class="toc-text">Encoder</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Decoder"><span class="toc-number">1.2.</span> <span class="toc-text">Decoder</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#模型细节"><span class="toc-number">2.</span> <span class="toc-text">模型细节</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Encoder-1"><span class="toc-number">2.1.</span> <span class="toc-text">Encoder</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#self-attention"><span class="toc-number">2.1.1.</span> <span class="toc-text">self-attention</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Multi-head"><span class="toc-number">2.1.2.</span> <span class="toc-text">Multi-head</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Position-encoding"><span class="toc-number">2.1.3.</span> <span class="toc-text">Position encoding</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Feed-Forward-NN层"><span class="toc-number">2.1.4.</span> <span class="toc-text">Feed Forward NN层</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Add-amp-Norm"><span class="toc-number">2.1.5.</span> <span class="toc-text">Add &amp; Norm</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#总架构图"><span class="toc-number">2.1.6.</span> <span class="toc-text">总架构图</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Decoder-1"><span class="toc-number">2.2.</span> <span class="toc-text">Decoder</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Linear层和softmax层"><span class="toc-number">2.2.1.</span> <span class="toc-text">Linear层和softmax层</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#两张有助于理解的图"><span class="toc-number">2.3.</span> <span class="toc-text">两张有助于理解的图</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Mask"><span class="toc-number">2.4.</span> <span class="toc-text">Mask</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Padding-mask"><span class="toc-number">2.4.1.</span> <span class="toc-text">Padding mask</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Sequence-mask"><span class="toc-number">2.4.2.</span> <span class="toc-text">Sequence mask</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Loss-Function"><span class="toc-number">2.5.</span> <span class="toc-text">Loss Function</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#beam-search（束搜索）"><span class="toc-number">2.5.1.</span> <span class="toc-text">beam search（束搜索）</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#参考"><span class="toc-number">3.</span> <span class="toc-text">参考</span></a></li></ol>
        
    </div>
    <style>
        .left-col .switch-btn,
        .left-col .switch-area {
            display: none;
        }
        .toc-level-4 i,
        .toc-level-4 ol {
            display: none !important;
        }
    </style>

    <input type="button" id="tocButton" value="隐藏目录"  title="点击按钮隐藏或者显示文章目录">

    <script>
        yiliaConfig.toc = ["隐藏目录", "显示目录", !!"false"];
    </script>





    
        <section id="comments" style="margin: 2em; padding: 2em; background: rgba(255, 255, 255, 0.5)">
    <div id="vcomment" class="comment"></div>
    <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
    <script src="//unpkg.com/valine@1.2.0-beta1/dist/Valine.min.js"></script>
    <script>
      new Valine({
        el: '#vcomment',
        notify: 'true',
        verify: 'true',
        app_id: "JbEkmNyN4oduuYuIVapp3mp7-gzGzoHsz",
        app_key: "lEKLgYFFECP8JrDYvUedV0z7",
        placeholder: "Just do it!",
        avatar: "mp"
      });
    </script>
</section>
    




    <div class="scroll" id="post-nav-button">
        
            <a href="/2019/02/python-json.html" title="上一篇: 利用Python处理json文件">
                <i class="fa fa-angle-left"></i>
            </a>
        

        <a title="文章列表"><i class="fa fa-bars"></i><i class="fa fa-times"></i></a>

        
            <a href="/2019/01/os-walk.html" title="下一篇: Python递归处理目录下的文件">
                <i class="fa fa-angle-right"></i>
            </a>
        
    </div>

    <ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2019/02/python-dict-list-trouble.html">Python中dict的值为list的问题</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/02/python-excel.html">Python处理Excel文件(csv, xls, xlsx)</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/02/python-json.html">利用Python处理json文件</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/01/transformer.html">The Transformer</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/01/os-walk.html">Python递归处理目录下的文件</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/01/print-colorful.html">Python打印带颜色的字符串</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/chmod.html">Linux文件权限管理命令chmod</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/monitorix-glances.html">monitorix--服务器监控程序安装及配置</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/anaconda.html">Anaconda快速使用教程</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/ssh-timeout.html">ssh连接服务器超时解决方案</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/tmux.html">强大的tmux</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/screen.html">Linux多重视窗管理命令screen</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/tar-gzip-zip-rar.html">Linux压缩和解压命令：tar，gzip，zip（unzip），rar</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/wget.html">Linux下载命令wget</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/ssr.html">SSR搭建教程</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/nohup.html">nohup：在Linux服务器后台跑程序</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/10/ps-grep-tail.html">Linux三个命令：ps，grep，tail</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/10/linear-classification4.html">线性分类模型(四)——贝叶斯观点下的Logistic回归</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/10/linear-classification3.html">线性分类模型(三)——判别式模型</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/10/argparse.html">Python命令行解析器argparse的使用</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/10/git.html">git常用命令</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/10/list-index.html">Python中list的查找</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/10/hexo-latex.html">Hexo中的LaTex公式渲染问题</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/10/linear-classification2.html">线性分类模型(二)——生成式模型</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/10/linear-classification1.html">线性分类模型(一)——线性判别函数</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/09/re.html">Python中正则表达式的特殊字符</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/09/valine.html">评论系统从Disqus到Valine</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/09/hexo.html">从Hugo到Hexo</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/09/mihomegiftbag.html">小米笔试题「小米大礼包」Python代码</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/09/hugo.html">使用HUGO搭建个人博客</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/09/index.html">Hello World</a></li></ul>




    <script>
        
    </script>
</div>
      <footer id="footer">
    <div class="outer">
        <div id="footer-info">
            <div class="footer-left">
                <i class="fa fa-copyright"></i> 
                2018-2019 Suixin
            </div>
        </div>
        
            <div class="visit">
                
                    <span id="busuanzi_container_site_pv" style='display:none'>
                        <span id="site-visit" title="本站到访数"><i class="fa fa-user" aria-hidden="true"></i><span id="busuanzi_value_site_uv"></span>
                        </span>
                    </span>
                
                
                    <span>| </span>
                
                
                    <span id="busuanzi_container_page_pv" style='display:none'>
                        <span id="page-visit"  title="本页阅读量"><i class="fa fa-eye animated infinite pulse" aria-hidden="true"></i><span id="busuanzi_value_page_pv"></span>
                        </span>
                    </span>
                
            </div>
        
        <br>
        <span id="timeDate">载入天数...</span><span id="times">载入时分秒...</span>
        <script>
            var now = new Date(); 
            function createtime() { 
                var grt= new Date("09/24/2018 13:14:00");//此处修改你的建站时间或者网站上线时间 
                now.setTime(now.getTime()+250); 
                days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days); 
                hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours); 
                if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum); 
                mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;} 
                seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum); 
                snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;} 
                document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 "; 
                document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒"; 
            } 
            setInterval("createtime()",250);
        </script>
    </div>
</footer>
    </div>
    
<script data-main="/js/main.js" src="//cdn.bootcss.com/require.js/2.2.0/require.min.js"></script>

    <script>
        $(document).ready(function() {
            var iPad = window.navigator.userAgent.indexOf('iPad');
            if (iPad > -1 || $(".left-col").css("display") === "none") {
                var bgColorList = ["#9db3f4", "#414141", "#e5a859", "#f5dfc6", "#c084a0", "#847e72", "#cd8390", "#996731"];
                var bgColor = Math.ceil(Math.random() * (bgColorList.length - 1));
                $("body").css({"background-color": bgColorList[bgColor], "background-size": "cover"});
            }
            else {
                var backgroundnum = 6;
                var backgroundimg = "url(/background/bg-x.jpg)".replace(/x/gi, Math.ceil(Math.random() * backgroundnum));
                $("body").css({"background": backgroundimg, "background-attachment": "fixed", "background-size": "cover"});
            }
        })
    </script>





    <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script src="//cdn.bootcss.com/mathjax/2.6.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<div class="scroll" id="scroll">
    <a href="#" title="返回顶部"><i class="fa fa-arrow-up"></i></a>
    <a href="#comments" onclick="load$hide();" title="查看评论"><i class="fa fa-comments-o"></i></a>
    <a href="#footer" title="转到底部"><i class="fa fa-arrow-down"></i></a>
</div>
<script>
    // Open in New Window
    
        var oOpenInNew = {
            
            
            
            
            
            
             archives: ".archive-article-title", 
             miniArchives: "a.post-list-link", 
            
             friends: "#js-friends a", 
             socail: ".social a" 
        }
        for (var x in oOpenInNew) {
            $(oOpenInNew[x]).attr("target", "_blank");
        }
    
</script>

    <script>
        var originTitle = document.title;
        var titleTime;
        document.addEventListener("visibilitychange", function() {
            if (document.hidden) {
                document.title = "(つェ⊂) 我藏好了哦~ " + originTitle;
                clearTimeout(titleTime);
            }
            else {
                document.title = "(*´∇｀*) 被你发现啦~ " + originTitle;
                titleTime = setTimeout(function() {
                    document.title = originTitle;
                }, 2000);
            }
        })
    </script>

<script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
  </div>
</body>
</html>